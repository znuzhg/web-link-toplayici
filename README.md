# ğŸ•¸ï¸ Python Web TarayÄ±cÄ± (EÄŸitim Projesi)

Bu repo, Python ile web scraping ve temel web tarayÄ±cÄ± (crawler) mantÄ±ÄŸÄ±nÄ± Ã¶ÄŸrenmek iÃ§in adÄ±m adÄ±m geliÅŸtirilmiÅŸ bir projedir.  
AmaÃ§, Ã¶nce en basit seviyeden baÅŸlayarak sonra multi-threading kullanarak daha hÄ±zlÄ± ve geliÅŸmiÅŸ bir tarayÄ±cÄ± oluÅŸturmaktÄ±r.

Bu proje 3 aÅŸamada evrilmiÅŸtir:

---

## ğŸ“Œ **V1 â€“ Basit Seviye: Tek Sayfadan Link Toplama**

Bu aÅŸamada sadece bir web sayfasÄ±na istek gÃ¶nderilip `<a>` etiketleri iÃ§erisindeki `href` deÄŸerleri toplanmÄ±ÅŸtÄ±r.

- Sadece tek sayfa taranÄ±r  
- `BeautifulSoup` ile HTML parse edilir  
- `http` ile baÅŸlayan linkler filtrelenir  
- Basit, giriÅŸ seviyesi scraping Ã¶rneÄŸidir  

Kodun amacÄ±: **Web scraping mantÄ±ÄŸÄ±nÄ± anlamak.**

---

## ğŸ“Œ **V2 â€“ Orta Seviye: Threadsiz Ã‡ok Sayfa TarayÄ±cÄ±sÄ±**

Bu aÅŸamada:

- Ana sayfadaki linkler toplanÄ±r  
- Bulunan her linke tekrar istek gÃ¶nderilir  
- Her sayfadaki linkler tek tek Ã§Ä±karÄ±lÄ±r  
- Duplicate (tekrar eden) linkler temizlenir  
- TÃ¼m iÅŸlem *tek thread* Ã¼zerinde yapÄ±lÄ±r â†’ yavaÅŸtÄ±r  

Bu sÃ¼rÃ¼m, gerÃ§ek bir tarayÄ±cÄ±nÄ±n Ã§alÄ±ÅŸma mantÄ±ÄŸÄ±nÄ± gÃ¶sterir ancak performans dÃ¼ÅŸÃ¼ktÃ¼r.

Kodun amacÄ±:  
**Ã‡oklu sayfa taramayÄ± Ã¶ÄŸrenmek ve crawler mantÄ±ÄŸÄ±nÄ± kavramak.**

---

## ğŸ“Œ **V3 â€“ GeliÅŸmiÅŸ Seviye: Multi-thread Web TarayÄ±cÄ± (HÄ±zlÄ±)**

Bu aÅŸama projeyi hÄ±zlandÄ±rmak iÃ§in Ã§ok iÅŸ parÃ§acÄ±ÄŸÄ± (thread) kullanÄ±r.

YapÄ±lan geliÅŸtirmeler:

- Ana sayfadaki linkler toplanÄ±r  
- Her link iÃ§in ayrÄ± bir thread oluÅŸturulur  
- Threadâ€™ler aynÄ± anda Ã§alÄ±ÅŸarak Ã§ok daha hÄ±zlÄ± tarama yapar  
- `threading.Lock()` ile thread-safe liste yazÄ±mÄ± saÄŸlanÄ±r  
- Duplicate linkler `dict.fromkeys()` ile temizlenir  
- SÃ¼re Ã¶lÃ§Ã¼mÃ¼ eklenmiÅŸtir (`time()`)

Bu sÃ¼rÃ¼m, **gerÃ§ek bir web tarayÄ±cÄ±sÄ±nÄ±n performanslÄ± versiyonudur.**

Kodun amacÄ±:  
**Multi-threading mantÄ±ÄŸÄ±nÄ± Ã¶ÄŸrenmek ve web tarayÄ±cÄ±sÄ±nÄ± hÄ±zlandÄ±rmak.**

---

## ğŸ› ï¸ KullanÄ±lan Teknolojiler

| Teknoloji | AÃ§Ä±klama |
|----------|----------|
| `requests` | HTTP istekleri iÃ§in |
| `BeautifulSoup (bs4)` | HTML parse etmek iÃ§in |
| `threading` | Paralel tarama yapmak iÃ§in |
| `time` | Basit sÃ¼re Ã¶lÃ§Ã¼mÃ¼ iÃ§in |

---

## ğŸš€ Ã‡alÄ±ÅŸtÄ±rmak Ä°Ã§in

1. Gerekli paketleri yÃ¼kleyin:
   ```bash
   pip install requests beautifulsoup4
   ```
2) Projeyi bilgisayarÄ±nÄ±za indirin

GitHub Ã¼zerinden:
```bash
git clone https://github.com/znuzhg/web-link-toplayici.git
```

veya ZIP olarak indirip klasÃ¶re Ã§Ä±karabilirsiniz.

3) Proje klasÃ¶rÃ¼ne gidin
```bash
cd REPO_ADI
```
5) Python dosyasÄ±nÄ± Ã§alÄ±ÅŸtÄ±rÄ±n
```bash
python main.py
```
---
7) Ã‡Ä±ktÄ±yÄ± inceleyin
---
Program Ã§alÄ±ÅŸtÄ±ÄŸÄ±nda terminalde:

Toplam bulunan link sayÄ±sÄ±

Linklerin kendisi

Tarama sÃ¼resi (sn)

gÃ¶rÃ¼ntÃ¼lenecektir.
---
ğŸ¯ Ã–ÄŸrenme Hedefleri
---
Bu proje sayesinde:
---
Web scraping nedir, nasÄ±l yapÄ±lÄ±r?

HTML nasÄ±l parse edilir?

Linkler nasÄ±l filtrelenir?

Duplicate link nasÄ±l temizlenir?

Crawler nasÄ±l Ã§alÄ±ÅŸÄ±r?

Threading ile performans nasÄ±l artÄ±rÄ±lÄ±r?

SorularÄ±na pratik yanÄ±t veren somut bir proje elde edilmiÅŸ oldu.
---
---
ğŸ“Œ GeliÅŸtirme PlanÄ± (Ä°steÄŸe BaÄŸlÄ±)
---
Ä°leride projeye ÅŸunlar eklenebilir:
---
domain filtering (sadece site iÃ§i linkleri tarama)

depth limit (2. seviye, 3. seviye tarama)

robots.txt uyumluluÄŸu

ThreadPoolExecutor ile daha temiz threading

asyncio + aiohttp ile ultra hÄ±zlÄ± tarama

JSON / CSV Ã§Ä±ktÄ± sistemi
---
---
ğŸ‘¤ GeliÅŸtiren
Bu proje eÄŸitim amaÃ§lÄ±dÄ±r ve Python Ã¶ÄŸrenme sÃ¼recinde adÄ±m adÄ±m geliÅŸtirilmiÅŸtir.

HazÄ±rlayan: Znuzhg Onyvxpv
---
